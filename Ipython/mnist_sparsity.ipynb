{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_sparsity.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1S0zGJ9Mfqros6Mjcre_PLN3f2jq_L0VE",
          "timestamp": 1518035508377
        },
        {
          "file_id": "1iEfLE4FmkiVQ8nx4jUYr6KAuxKTCp5UT",
          "timestamp": 1516826348243
        },
        {
          "file_id": "1IHNqHpSS3WQdTnIAOhWcN0wTjxpRayGT",
          "timestamp": 1516746341574
        },
        {
          "file_id": "17C-t9NvJevuqTAQUEkCpqM50UdCQVIpG",
          "timestamp": 1516601523439
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rp0URWpjJ3mn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "import functools\n",
        "from apiclient import errors\n",
        "from apiclient.http import MediaFileUpload\n",
        "# from IPython.core.debugger import set_trace\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5XpL0u7v-RmF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sparse_fn(weights,threshold=0.001):\n",
        "    # Force weights less than a threshold to zero.\n",
        "    # Set threshold and clip\n",
        "    # This force the weights in range [-threshold,threshold] to be zero.\n",
        "    W_signed = tf.sign(weights)\n",
        "    W_sparse_temp = tf.clip_by_value(tf.subtract(tf.abs(weights),threshold),clip_value_min=0,clip_value_max=10000.0)\n",
        "    W_sparse_signed = tf.sign(W_sparse_temp)\n",
        "    W_sparse = W_signed * W_sparse_signed * tf.add(W_sparse_temp,threshold)\n",
        "    return W_sparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iiHHYXgbK5eu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            },
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 2864
        },
        "outputId": "e19ad929-1f47-4956-9ad7-981e4d19ee55",
        "executionInfo": {
          "status": "error",
          "timestamp": 1518052182473,
          "user_tz": 300,
          "elapsed": 4398,
          "user": {
            "displayName": "amirsina torfi",
            "photoUrl": "//lh3.googleusercontent.com/-eprf1c04IO4/AAAAAAAAAAI/AAAAAAAAAB0/Kn35CXC21ck/s50-c-k-no/photo.jpg",
            "userId": "104221844145763989827"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Import data\n",
        "    mnist = input_data.read_data_sets(FLAGS.data_dir,\n",
        "                                      fake_data=FLAGS.fake_data)\n",
        "\n",
        "    sess = tf.InteractiveSession()\n",
        "    # Create a multilayer model.\n",
        "\n",
        "    # Input placeholders\n",
        "    with tf.name_scope('input'):\n",
        "        x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
        "        y_ = tf.placeholder(tf.int64, [None], name='y-input')\n",
        "\n",
        "    with tf.name_scope('input_reshape'):\n",
        "        image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
        "        tf.summary.image('input', image_shaped_input, 10)\n",
        "    \n",
        "    with tf.name_scope('dropout'):\n",
        "        keep_prob = tf.placeholder(tf.float32)\n",
        "        tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
        "    \n",
        "    with tf.name_scope('training_status'):\n",
        "        training_status = tf.placeholder(tf.bool)\n",
        "\n",
        "    # We can't initialize these variables to 0 - the network will get stuck.\n",
        "    def weight_variable(shape):\n",
        "        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
        "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "        return tf.Variable(initial)\n",
        "\n",
        "    def bias_variable(shape):\n",
        "        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
        "        initial = tf.constant(0.1, shape=shape)\n",
        "        return tf.Variable(initial)\n",
        "\n",
        "    def group_lasso(v):\n",
        "\n",
        "        group_loss_all = []\n",
        "        for W in v:\n",
        "            if 'bias' not in W.name:\n",
        "                if 'conv' in W.name:\n",
        "                    # Input-channel-wise sparsity\n",
        "                    grouped_sum = tf.sqrt(tf.reduce_sum(tf.pow(W,2),axis=[0,1,2]))\n",
        "                    group_loss = tf.reduce_sum(grouped_sum)\n",
        "                    group_loss_all.append(group_loss)\n",
        "                if 'fc' in W.name:\n",
        "                    # Input-channel-wise sparsity\n",
        "                    grouped_sum = tf.sqrt(tf.reduce_sum(tf.pow(W,2),axis=[0]))\n",
        "                    group_loss = tf.reduce_sum(grouped_sum)\n",
        "                    group_loss_all.append(group_loss)\n",
        "\n",
        "        return tf.reduce_sum(group_loss_all)\n",
        "     \n",
        "    \n",
        "    def sparsity_calculatior(v):\n",
        "      \n",
        "      # Calculation of the sparsity of the network\n",
        "      sparsity_layers = []\n",
        "      num_params_layers = []\n",
        "      for W in v:\n",
        "            if 'bias' not in W.name:\n",
        "                if 'conv' or 'fc' in W.name:\n",
        "                \n",
        "                    # Set threshold and clip\n",
        "                    W_sparse = tf.clip_by_value(tf.subtract(tf.abs(W),FLAGS.sparsity_threshold),clip_value_min=0,clip_value_max=10000)\n",
        "    \n",
        "                    # Sparsity calculation\n",
        "                    num_nonzero = tf.cast(tf.count_nonzero(W_sparse),tf.float32)\n",
        "                    num_weights = functools.reduce(lambda x, y: x*y, W_sparse.get_shape())\n",
        "                    non_sparsity_level = tf.divide(num_nonzero,tf.cast(num_weights,tf.float32))\n",
        "                    Sparsity = tf.subtract(1.0,non_sparsity_level)\n",
        "                    \n",
        "                    # Add the sparsity of each layer to the list\n",
        "                    sparsity_layers.append(Sparsity)\n",
        "                    \n",
        "                    # Add the number of parameters for each layer\n",
        "                    num_params_layers.append(num_nonzero)\n",
        "                 \n",
        "       \n",
        "      return tf.reduce_mean(sparsity_layers), tf.reduce_sum(num_params_layers)\n",
        "                \n",
        "      \n",
        "\n",
        "    def group_variance(v):\n",
        "        group_loss_variance = []\n",
        "        for W in v:\n",
        "            if 'bias' not in W.name:\n",
        "                if 'conv' in W.name:\n",
        "                    grouped_elements = tf.reduce_sum(tf.pow(W,2),axis=[0,1,2])\n",
        "                    coefficient=1.0\n",
        "                    group_mean, group_variance = tf.nn.moments(grouped_elements, axes=[0])\n",
        "                    variance_loss = tf.divide(1.0, tf.divide(group_variance, coefficient))\n",
        "                    group_loss_variance.append(variance_loss)\n",
        "                if 'fc' in W.name:\n",
        "                    grouped_elements = tf.reduce_sum(tf.pow(W,2),axis=[0])\n",
        "                    coefficient=1.0\n",
        "                    group_mean, group_variance = tf.nn.moments(grouped_elements, axes=[0])\n",
        "                    variance_loss = tf.divide(1.0, tf.divide(group_variance, coefficient))\n",
        "                    group_loss_variance.append(variance_loss)\n",
        "\n",
        "        return tf.reduce_sum(group_loss_variance)\n",
        "\n",
        "    def variable_summaries(var):\n",
        "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "        with tf.name_scope('summaries'):\n",
        "            mean = tf.reduce_mean(var)\n",
        "            tf.summary.scalar('mean', mean)\n",
        "            with tf.name_scope('stddev'):\n",
        "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "            tf.summary.scalar('stddev', stddev)\n",
        "            tf.summary.scalar('max', tf.reduce_max(var))\n",
        "            tf.summary.scalar('min', tf.reduce_min(var))\n",
        "\n",
        "    def conv2d(x, W, padding='SAME'):\n",
        "        \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
        "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=padding)\n",
        "\n",
        "    def max_pool_2x2(x):\n",
        "        \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
        "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                              strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "\n",
        "    def nn_layer(input_tensor, input_dim, output_dim, layer_name, training_status=True, act=tf.nn.relu):\n",
        "        \"\"\"Reusable code for making a simple neural net layer.\n",
        "\n",
        "        It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n",
        "        It also sets up name scoping so that the resultant graph is easy to read,\n",
        "        and adds a number of summary ops.\n",
        "        \"\"\"\n",
        "        # Adding a name scope ensures logical grouping of the layers in the graph.\n",
        "        with tf.name_scope(layer_name):\n",
        "            # This Variable will hold the state of the weights for the layer\n",
        "            with tf.name_scope('weights'):\n",
        "                weights = weight_variable([input_dim, output_dim])\n",
        "\n",
        "                # Get the general summaries\n",
        "                variable_summaries(weights)\n",
        "            with tf.name_scope('biases'):\n",
        "                biases = bias_variable([output_dim])\n",
        "                variable_summaries(biases)\n",
        "            with tf.name_scope('Wx_plus_b'):\n",
        "                \n",
        "                # At evaluation time, some weights are forced to be zero with the sparsity criterion.\n",
        "                weights = tf.cond(training_status,\n",
        "                           true_fn = lambda: sparse_fn(weights,threshold=0.0),\n",
        "                           false_fn = lambda: sparse_fn(weights,threshold=FLAGS.sparsity_threshold))\n",
        "                  \n",
        "                preactivate = tf.matmul(input_tensor, weights) + biases\n",
        "                # tf.summary.histogram('pre_activations', preactivate)\n",
        "\n",
        "            # Activation summary\n",
        "            activations = act(preactivate, name='activation')\n",
        "            tf.summary.scalar('sparsity', tf.nn.zero_fraction(activations))\n",
        "            tf.summary.histogram('activations', activations)\n",
        "\n",
        "\n",
        "            # Overall neurons\n",
        "            neurons = tf.reduce_sum(tf.abs(weights), axis=1)\n",
        "            tf.summary.histogram('neurons', neurons)\n",
        "\n",
        "\n",
        "            return activations\n",
        "\n",
        "    def nn_conv_layer(input_tensor, w_shape, b_shape, layer_name, padding='SAME', training_status=True, act=tf.nn.relu):\n",
        "        \"\"\"Reusable code for making a simple neural net layer.\n",
        "\n",
        "        It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n",
        "        It also sets up name scoping so that the resultant graph is easy to read,\n",
        "        and adds a number of summary ops.\n",
        "        \"\"\"\n",
        "        # Adding a name scope ensures logical grouping of the layers in the graph.\n",
        "        with tf.name_scope(layer_name):\n",
        "            # This Variable will hold the state of the weights for the layer\n",
        "            with tf.name_scope('weights'):\n",
        "                weights = weight_variable(w_shape)\n",
        "                variable_summaries(weights)\n",
        "            with tf.name_scope('biases'):\n",
        "                biases = bias_variable(b_shape)\n",
        "                variable_summaries(biases)\n",
        "            with tf.name_scope('Wx_plus_b'):\n",
        "                \n",
        "                # At evaluation time, some weights are forced to be zero with the sparsity criterion.\n",
        "                weights = tf.cond(training_status,\n",
        "                           true_fn = lambda: sparse_fn(weights,threshold=0.0),\n",
        "                           false_fn = lambda: sparse_fn(weights,threshold=FLAGS.sparsity_threshold))\n",
        "                    \n",
        "                preactivate = conv2d(input_tensor, weights,padding) + biases\n",
        "                # tf.summary.histogram('pre_activations', preactivate)\n",
        "                \n",
        "            activations = act(preactivate, name='activation')\n",
        "            # tf.summary.histogram('activations', activations)\n",
        "            return activations\n",
        "\n",
        "    def net(x,training_status):\n",
        "\n",
        "        with tf.name_scope('reshape'):\n",
        "            x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "        h_conv1 = nn_conv_layer(x_image, [5, 5, 1, 64], [64], 'conv1', training_status=training_status, act=tf.nn.relu)\n",
        "\n",
        "        with tf.name_scope('pool1'):\n",
        "            h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "        h_conv2 = nn_conv_layer(h_pool1, [5, 5, 64, 128], [128], 'conv2', training_status=training_status, act=tf.nn.relu)\n",
        "\n",
        "        # Second pooling layer.\n",
        "        with tf.name_scope('pool2'):\n",
        "            h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 128])\n",
        "        \n",
        "        h_fc1 = nn_layer(h_pool2_flat, 7 * 7 * 128, 512, 'fc1', training_status=training_status, act=tf.nn.relu)\n",
        "        dropped_h_fc1 = tf.nn.dropout(h_fc1, keep_prob)\n",
        "            \n",
        "        h_fc2 = nn_layer(dropped_h_fc1, 512, 256, 'fc2', training_status=training_status, act=tf.nn.relu)\n",
        "        dropped_h_fc2 = tf.nn.dropout(h_fc2, keep_prob)\n",
        "\n",
        "        # Do not apply softmax activation yet, see below.\n",
        "        output = nn_layer(dropped_h_fc2, 256, 10, 'softmax', training_status=training_status, act=tf.identity)\n",
        "\n",
        "        return output, keep_prob\n",
        "\n",
        "    # Network\n",
        "    output, keep_prob = net(x,training_status)\n",
        "\n",
        "    with tf.name_scope('cross_entropy'):\n",
        "        with tf.name_scope('total'):\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n",
        "                labels=y_, logits=output)\n",
        "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
        "\n",
        "    #############################\n",
        "    ########## LOSS #############\n",
        "    #############################\n",
        "\n",
        "    # Get all trainable variables except biases\n",
        "    trainable_variables = tf.trainable_variables()\n",
        "\n",
        "    # Compute the regularization term\n",
        "    with tf.name_scope('group_lasso'):\n",
        "        lasso_loss = 0.001 * group_lasso(trainable_variables)\n",
        "\n",
        "    with tf.name_scope('group_variance'):\n",
        "        variance_loss = 0.01 * group_variance(trainable_variables)\n",
        "\n",
        "    with tf.name_scope('group_lasso_invert'):\n",
        "        lasso_loss_invert = 10.0 * tf.divide(1,group_lasso(trainable_variables))\n",
        "\n",
        "    tf.losses.add_loss(\n",
        "        lasso_loss,\n",
        "        loss_collection=tf.GraphKeys.LOSSES\n",
        "    )\n",
        "\n",
        "    tf.losses.add_loss(\n",
        "        variance_loss,\n",
        "        loss_collection=tf.GraphKeys.LOSSES\n",
        "    )\n",
        "\n",
        "    # Compute the regularization term\n",
        "    with tf.name_scope('Sparsity'):\n",
        "        sparsity, num_params = sparsity_calculatior(trainable_variables)\n",
        "    \n",
        "    tf.summary.scalar('sparsity', sparsity)\n",
        "\n",
        "    ###############################################\n",
        "    ############### Total Loss  ###################\n",
        "    ###############################################\n",
        "\n",
        "    total_loss = tf.losses.get_total_loss(add_regularization_losses=True, name='total_loss')\n",
        "    list_losses = tf.losses.get_losses(loss_collection=tf.GraphKeys.LOSSES)\n",
        "    reg_losses = tf.losses.get_losses(loss_collection=tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "\n",
        "\n",
        "    with tf.name_scope('train'):\n",
        "        train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n",
        "            total_loss)\n",
        "\n",
        "    with tf.name_scope('accuracy'):\n",
        "        with tf.name_scope('correct_prediction'):\n",
        "            correct_prediction = tf.equal(tf.argmax(output, 1), y_)\n",
        "        with tf.name_scope('accuracy'):\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "    # Merge all the summaries and write them out to\n",
        "    # /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n",
        "    merged = tf.summary.merge_all()\n",
        "    train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)\n",
        "    test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test')\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    # Train the model, and also write summaries.\n",
        "    def feed_dict(train):\n",
        "        \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
        "        if train or FLAGS.fake_data:\n",
        "            is_train = True\n",
        "            xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n",
        "            k = FLAGS.dropout\n",
        "        else:\n",
        "            is_train = False\n",
        "            xs, ys = mnist.test.next_batch(1000)\n",
        "            k = 1.0\n",
        "        return {x: xs, y_: ys, keep_prob: k, training_status:is_train}\n",
        "\n",
        "    for i in range(1, FLAGS.max_steps):\n",
        "\n",
        "        if i % 100 == 0:  # Record summaries and test-set accuracy\n",
        "            summary, acc,sparsity_value, num_parameters = sess.run([merged, accuracy,sparsity, num_params], feed_dict=feed_dict(False))\n",
        "            test_writer.add_summary(summary, i)\n",
        "            print('Accuracy and Sparsity at step %s: %s , %s\\n number of parameters= %s' % (i, acc, sparsity_value,num_parameters))\n",
        "\n",
        "\n",
        "        else:  # Record a summary\n",
        "            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
        "            train_writer.add_summary(summary, i)\n",
        "            \n",
        "    train_writer.close()\n",
        "    test_writer.close()\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    if tf.gfile.Exists(FLAGS.log_dir):\n",
        "        tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
        "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
        "    train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--fake_data', nargs='?', const=True, type=bool,\n",
        "                        default=False,\n",
        "                        help='If true, uses fake data for unit testing.')\n",
        "    parser.add_argument('--max_steps', type=int, default=300000,\n",
        "                        help='Number of steps to run trainer.')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.0001,\n",
        "                        help='Initial learning rate')\n",
        "    parser.add_argument('--sparsity_threshold', type=float, default=0.001,\n",
        "                        help='Initial learning rate')\n",
        "    parser.add_argument('--dropout', type=float, default=0.8,\n",
        "                        help='Keep probability for training dropout.')\n",
        "    parser.add_argument(\n",
        "        '--data_dir',\n",
        "        type=str,\n",
        "        default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
        "                             'tensorflow/mnist/input_data'),\n",
        "        help='Directory for storing input data')\n",
        "    parser.add_argument(\n",
        "        '--log_dir',\n",
        "        type=str,\n",
        "        default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
        "                             'tensorflow/mnist/logs/mnist_sparsity'),\n",
        "        help='Summaries log directory')\n",
        "    FLAGS, unparsed = parser.parse_known_args()\n",
        "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
        "    \n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2002\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2003\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No attr named '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"' in \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2004\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No attr named '_XlaCompile' in name: \"fc1_1/Wx_plus_b/cond/mul_2\"\nop: \"Mul\"\ninput: \"fc1_1/Wx_plus_b/cond/Sign_2\"\ninput: \"fc1_1/Wx_plus_b/cond/Sign_3\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9b78bb277fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    367\u001b[0m         help='Summaries log directory')\n\u001b[1;32m    368\u001b[0m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0munparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9b78bb277fc3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeleteRecursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9b78bb277fc3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n\u001b[0;32m--> 292\u001b[0;31m             total_loss)\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         colocate_gradients_with_ops=colocate_gradients_with_ops)\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_GRAPH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                 in_grads = _MaybeCompile(\n\u001b[0;32m--> 581\u001b[0;31m                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    582\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                 in_grads = _MaybeCompile(\n\u001b[0;32m--> 581\u001b[0;31m                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    582\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" vs. \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m   \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    212\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[1;32m    213\u001b[0m              \"dtype\": dtype_value},\n\u001b[0;32m--> 214\u001b[0;31m       name=name).outputs[0]\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2954\u001b[0m         \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2955\u001b[0m         \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2956\u001b[0;31m         op_def=op_def)\n\u001b[0m\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1438\u001b[0m     self._outputs = [\n\u001b[1;32m   1439\u001b[0m         \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m     ]\n\u001b[1;32m   1442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_types\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1438\u001b[0m     self._outputs = [\n\u001b[1;32m   1439\u001b[0m         \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m     ]\n\u001b[1;32m   1442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_types\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, op, value_index, dtype)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;31m# If set, will be a HandleData object from cpp_shape_inference.proto.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36muid\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0;34m\"\"\"A unique (within this program execution) integer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_Py_UID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HjwLIearFz1X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FCskNm71ReYW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}